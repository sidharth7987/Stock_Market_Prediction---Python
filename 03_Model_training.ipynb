{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b448cf-3b7e-4f82-acc9-f4ae5bfed447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "#              Model_Training\n",
    "# -------------------------------------------------\n",
    "\n",
    "\"\"\" \n",
    "Final production-ready script for next-day Open/Close forecasting.\n",
    "\n",
    "    Features:\n",
    "     - Loads engineered features CSV (expects 'Open' and 'Close' columns)\n",
    "     - Creates next-day targets: next_open, next_close\n",
    "     - Adds configurable lag features\n",
    "     - Standard scaling\n",
    "     - Walk-forward (expanding-window) cross-validation\n",
    "     - XGBoost training (recommended) with a robust fit call (works across xgboost versions)\n",
    "     - Optional Optuna hyperparameter tuning for XGBoost\n",
    "     - RandomForest baseline option\n",
    "     - Saves model artifacts and training summary\n",
    "     - Works inside Jupyter notebooks via parse_known_args()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa745eff-cb8f-4aa2-a1dc-63d92b2e6fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished target=next_close. Summary:\n",
      "{\n",
      "  \"cv_folds\": [\n",
      "    {\n",
      "      \"fold\": 0,\n",
      "      \"metrics\": {\n",
      "        \"mae\": 10.403113114206414,\n",
      "        \"rmse\": 13.011159313989314,\n",
      "        \"r2\": 0.9204854361254439,\n",
      "        \"mape\": 0.8674856048678252\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"fold\": 1,\n",
      "      \"metrics\": {\n",
      "        \"mae\": 95.48250379060444,\n",
      "        \"rmse\": 132.48770026458632,\n",
      "        \"r2\": -0.5170183556622374,\n",
      "        \"mape\": 6.660437696929019\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"fold\": 2,\n",
      "      \"metrics\": {\n",
      "        \"mae\": 21.839400442023027,\n",
      "        \"rmse\": 30.410088146436358,\n",
      "        \"r2\": 0.12059792776472666,\n",
      "        \"mape\": 1.4949401405752423\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"fold\": 3,\n",
      "      \"metrics\": {\n",
      "        \"mae\": 30.703370746813324,\n",
      "        \"rmse\": 41.514130605709205,\n",
      "        \"r2\": 0.5604184813886672,\n",
      "        \"mape\": 2.026895528753216\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"fold\": 4,\n",
      "      \"metrics\": {\n",
      "        \"mae\": 18.20243273283306,\n",
      "        \"rmse\": 23.21874105771824,\n",
      "        \"r2\": 0.6445653541759144,\n",
      "        \"mape\": 1.4185014042879882\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"test_metrics\": {\n",
      "    \"mae\": 13.699415989925987,\n",
      "    \"rmse\": 17.534786517412204,\n",
      "    \"r2\": 0.9653204869709503,\n",
      "    \"mape\": 0.9918648718693173\n",
      "  },\n",
      "  \"best_params\": {\n",
      "    \"n_estimators\": 500,\n",
      "    \"learning_rate\": 0.05,\n",
      "    \"max_depth\": 6,\n",
      "    \"subsample\": 0.8,\n",
      "    \"colsample_bytree\": 0.8,\n",
      "    \"random_state\": 42\n",
      "  },\n",
      "  \"saved\": {\n",
      "    \"model_path\": \"models\\\\next_close\\\\next_close_xgb_xgb.json\",\n",
      "    \"framework\": \"xgboost\",\n",
      "    \"scaler_path\": \"models\\\\next_close\\\\next_close_xgb_scaler.joblib\",\n",
      "    \"predictors_path\": \"models\\\\next_close\\\\next_close_xgb_predictors.json\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from typing import List, Optional, Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# Optional libraries\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except Exception:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "    OPTUNA_AVAILABLE = True\n",
    "except Exception:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "# --------------------- Utilities -----------------------------\n",
    "\n",
    "def load_features(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    if 'Date' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.sort_values('Date').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_next_day_targets(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if 'Open' not in df.columns or 'Close' not in df.columns:\n",
    "        raise ValueError(\"Input features must contain 'Open' and 'Close' columns.\")\n",
    "    df['next_open'] = df['Open'].shift(-1)\n",
    "    df['next_close'] = df['Close'].shift(-1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_lag_features(df: pd.DataFrame, cols: List[str], lags: List[int]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        for l in lags:\n",
    "            df[f\"{c}_lag_{l}\"] = df[c].shift(l)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_predictor_columns(df: pd.DataFrame, exclude: List[str]) -> List[str]:\n",
    "    exclude_set = set(exclude)\n",
    "    return [c for c in df.select_dtypes(include=[np.number]).columns if c not in exclude_set]\n",
    "\n",
    "\n",
    "def eval_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = np.mean((np.array(y_true) - np.array(y_pred)) ** 2)\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / np.where(y_true == 0, 1e-8, y_true))) * 100\n",
    "    return {'mae': float(mae), 'rmse': rmse, 'r2': float(r2), 'mape': float(mape)}\n",
    "\n",
    "\n",
    "# ----------------------- XGBoost Training ------------------------\n",
    "\n",
    "def train_xgb(X_train: np.ndarray, y_train: np.ndarray,\n",
    "              X_val: Optional[np.ndarray] = None, y_val: Optional[np.ndarray] = None,\n",
    "              params: Optional[Dict[str, Any]] = None) -> Any:\n",
    "    if not XGB_AVAILABLE:\n",
    "        raise RuntimeError('xgboost is not available. Install xgboost to use this trainer.')\n",
    "\n",
    "    params = params or {\n",
    "        'n_estimators': 500,\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': 6,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'random_state': SEED\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "\n",
    "    if X_val is not None and y_val is not None:\n",
    "        try:\n",
    "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=20, verbose=False)\n",
    "        except TypeError:\n",
    "            try:\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "            except TypeError:\n",
    "                model.fit(X_train, y_train)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ----------------- Walk-Forward CV --------------------------\n",
    "\n",
    "def walk_forward_split(n_samples: int, n_splits: int = 5, initial_train_size: Optional[int] = None) -> List[Tuple[slice, slice]]:\n",
    "    if initial_train_size is None:\n",
    "        initial_train_size = int(n_samples * 0.5)\n",
    "    test_size = int((n_samples - initial_train_size) / n_splits)\n",
    "    if test_size < 1:\n",
    "        test_size = 1\n",
    "    splits = []\n",
    "    train_end = initial_train_size\n",
    "    for i in range(n_splits):\n",
    "        test_start = train_end\n",
    "        test_end = min(test_start + test_size, n_samples)\n",
    "        if test_start >= n_samples:\n",
    "            break\n",
    "        splits.append((slice(0, train_end), slice(test_start, test_end)))\n",
    "        train_end = test_end\n",
    "        if test_end == n_samples:\n",
    "            break\n",
    "    return splits\n",
    "\n",
    "\n",
    "def run_walk_forward_cv(X: np.ndarray, y: np.ndarray, n_splits: int = 5, initial_train_size: Optional[int] = None,\n",
    "                        params: Optional[Dict[str, Any]] = None) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
    "    splits = walk_forward_split(len(X), n_splits=n_splits, initial_train_size=initial_train_size)\n",
    "    fold_results = []\n",
    "\n",
    "    for i, (train_slice, test_slice) in enumerate(splits):\n",
    "        X_tr, y_tr = X[train_slice], y[train_slice]\n",
    "        X_te, y_te = X[test_slice], y[test_slice]\n",
    "        model = train_xgb(X_tr, y_tr, X_val=X_te, y_val=y_te, params=params)\n",
    "        preds = model.predict(X_te)\n",
    "        metrics = eval_metrics(y_te, preds)\n",
    "        fold_results.append({'fold': i, 'metrics': metrics})\n",
    "    return fold_results, (params or {})\n",
    "\n",
    "\n",
    "# ------------------- Optuna Objective -------------------------\n",
    "\n",
    "def optuna_objective(trial, X: np.ndarray, y: np.ndarray, n_splits: int = 3, initial_train_size: Optional[int] = None) -> float:\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 10.0, log=True),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'random_state': SEED,\n",
    "    }\n",
    "\n",
    "    splits = walk_forward_split(len(X), n_splits=n_splits, initial_train_size=initial_train_size)\n",
    "    val_scores = []\n",
    "    for train_slice, test_slice in splits:\n",
    "        X_tr, y_tr = X[train_slice], y[train_slice]\n",
    "        X_te, y_te = X[test_slice], y[test_slice]\n",
    "        try:\n",
    "            model = xgb.XGBRegressor(**params)\n",
    "            model.fit(X_tr, y_tr, eval_set=[(X_te, y_te)], early_stopping_rounds=20, verbose=False)\n",
    "        except Exception:\n",
    "            model = xgb.XGBRegressor(**params)\n",
    "            model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_te)\n",
    "        metrics = eval_metrics(y_te, preds)\n",
    "        val_scores.append(metrics['rmse'])\n",
    "    return float(np.mean(val_scores))\n",
    "\n",
    "# ------------ Persistence -------------------\n",
    "\n",
    "def save_artifacts(model: Any, scaler: Optional[StandardScaler], out_dir: str, name: str, predictors: Optional[List[str]] = None) -> Dict[str, Any]:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    meta: Dict[str, Any] = {}\n",
    "    try:\n",
    "        if hasattr(model, 'get_booster'):\n",
    "            model.get_booster().save_model(os.path.join(out_dir, f\"{name}_xgb.json\"))\n",
    "            meta['model_path'] = os.path.join(out_dir, f\"{name}_xgb.json\")\n",
    "            meta['framework'] = 'xgboost'\n",
    "        else:\n",
    "            joblib.dump(model, os.path.join(out_dir, f\"{name}_model.joblib\"))\n",
    "            meta['model_path'] = os.path.join(out_dir, f\"{name}_model.joblib\")\n",
    "            meta['framework'] = 'joblib'\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to save model: {e}\")\n",
    "\n",
    "    if scaler is not None:\n",
    "        scaler_path = os.path.join(out_dir, f\"{name}_scaler.joblib\")\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "        meta['scaler_path'] = scaler_path\n",
    "\n",
    "    # save predictors list (ordered)\n",
    "    if predictors is not None:\n",
    "        try:\n",
    "            preds_path = os.path.join(out_dir, f\"{name}_predictors.json\")\n",
    "            with open(preds_path, 'w') as f:\n",
    "                json.dump(predictors, f)\n",
    "            meta['predictors_path'] = preds_path\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: failed to save predictors list: {e}\")\n",
    "\n",
    "    return meta\n",
    "\n",
    "\n",
    "# ------------------- Expanding-window one-step-ahead prediction-----------------------------------------\n",
    "\n",
    "def generate_expanding_one_step_predictions(df_features: pd.DataFrame, X_cols: List[str], target_col: str,\n",
    "                                            model_params: Dict[str, Any], initial_train_size: Optional[int] = None,\n",
    "                                            verbose: bool = True) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    For each time t starting from initial_train_size .. n-2:\n",
    "      - train on rows [0:t]\n",
    "      - predict target for row t (which corresponds to next-day of row t-1 in original target shift)\n",
    "    Returns DataFrame with Date (corresponding to the row being predicted) and prediction value.\n",
    "    \"\"\"\n",
    "\n",
    "    if initial_train_size is None:\n",
    "        initial_train_size = int(len(df_features) * 0.5)\n",
    "\n",
    "    n = len(df_features)\n",
    "    preds = []\n",
    "    dates = []\n",
    "\n",
    "    # Build full matrix of predictors (drop rows with NaNs produced by lagging will already be cleaned by caller)\n",
    "    X_all = df_features[X_cols].values\n",
    "    y_all = df_features[target_col].values\n",
    "\n",
    "    if n <= initial_train_size + 1:\n",
    "        raise ValueError(\"Not enough rows to generate expanding-window predictions with given initial_train_size.\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Generating expanding-window predictions: n={n}, initial_train_size={initial_train_size}\")\n",
    "\n",
    "    for t in range(initial_train_size, n - 0):  # we will predict for index t (which has a target value at t)\n",
    "        # train on [0:t)\n",
    "        train_end = t\n",
    "        X_tr = X_all[:train_end]\n",
    "        y_tr = y_all[:train_end]\n",
    "        # the prediction row is at index train_end (predict y_all[train_end])\n",
    "        X_pred = X_all[train_end].reshape(1, -1)\n",
    "\n",
    "        # train model quickly (no heavy val)\n",
    "        model = xgb.XGBRegressor(**model_params)\n",
    "        try:\n",
    "            model.fit(X_tr, y_tr, verbose=False)\n",
    "        except Exception:\n",
    "            model.fit(X_tr, y_tr)\n",
    "\n",
    "        pred = model.predict(X_pred)[0]\n",
    "        preds.append(float(pred))\n",
    "        dates.append(df_features['Date'].iloc[train_end] if 'Date' in df_features.columns else train_end)\n",
    "\n",
    "        # (optional) verbose progress\n",
    "        if verbose and (t - initial_train_size + 1) % 50 == 0:\n",
    "            print(f\"  predicted {t - initial_train_size + 1} / {n - initial_train_size} rows\")\n",
    "\n",
    "    out = pd.DataFrame({'Date': dates, f'pred_{target_col}': preds})\n",
    "    return out\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# ----------------- Main Pipeline ---------------------------\n",
    "\n",
    "def run_pipeline(\n",
    "    data_path: str,\n",
    "    lags: List[int],\n",
    "    model_type: str,\n",
    "    target: str,\n",
    "    test_size: float,\n",
    "    preserve_time: bool,\n",
    "    out_dir: str,\n",
    "    use_optuna: bool = False,\n",
    "    n_trials: int = 20,\n",
    "    n_splits: int = 5,\n",
    "    initial_train_size: Optional[int] = None,\n",
    "    generate_full_preds: bool = True,\n",
    "    fast_mode: bool = False\n",
    ") -> Dict[str, Any]:\n",
    "    df = load_features(data_path)\n",
    "    df = create_next_day_targets(df)\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    exclude = ['next_open', 'next_close']\n",
    "    base_cols = [c for c in ['Open', 'Close'] if c in numeric_cols]\n",
    "    other_cols = [c for c in numeric_cols if c not in base_cols + exclude]\n",
    "    lag_cols = base_cols + other_cols\n",
    "\n",
    "    df = add_lag_features(df, lag_cols, lags)\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "    targets = []\n",
    "    if target == 'next_open':\n",
    "        targets = ['next_open']\n",
    "    elif target == 'next_close':\n",
    "        targets = ['next_close']\n",
    "    elif target == 'both':\n",
    "        targets = ['next_open', 'next_close']\n",
    "\n",
    "    summary = {}\n",
    "\n",
    "    for t in targets:\n",
    "        X_cols = get_predictor_columns(df, exclude=[t])\n",
    "        X = df[X_cols].values\n",
    "        y = df[t].values\n",
    "\n",
    "        split_idx = int(len(X) * (1 - test_size))\n",
    "        X_train_all, X_test = X[:split_idx], X[split_idx:]\n",
    "        y_train_all, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_all_s = scaler.fit_transform(X_train_all)\n",
    "        X_test_s = scaler.transform(X_test)\n",
    "\n",
    "        model_results = {}\n",
    "\n",
    "        if model_type in ('xgb', 'all'):\n",
    "            if not XGB_AVAILABLE:\n",
    "                print('xgboost not installed â€” skipping xgb training')\n",
    "            else:\n",
    "                best_params = None\n",
    "                if use_optuna and OPTUNA_AVAILABLE:\n",
    "                    print('Running Optuna tuning (this may take a while)...')\n",
    "                    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "                    study.optimize(lambda trial: optuna_objective(trial, X_train_all_s, y_train_all, n_splits=max(2, n_splits//2), initial_train_size=initial_train_size), n_trials=n_trials)\n",
    "                    best_params = study.best_params\n",
    "                    print('Optuna best params:', best_params)\n",
    "                elif use_optuna and not OPTUNA_AVAILABLE:\n",
    "                    print('Optuna not installed. Running with default xgboost params.')\n",
    "\n",
    "                xgb_params = best_params or {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 6, 'subsample': 0.8, 'colsample_bytree': 0.8, 'random_state': SEED}\n",
    "\n",
    "                # Evaluate with walk-forward CV\n",
    "                folds, _ = run_walk_forward_cv(X_train_all_s, y_train_all, n_splits=n_splits, initial_train_size=initial_train_size, params=xgb_params)\n",
    "\n",
    "                # Train final model on full training set using chosen params\n",
    "                final_model = train_xgb(X_train_all_s, y_train_all, params=xgb_params)\n",
    "                preds_test = final_model.predict(X_test_s)\n",
    "                test_metrics = eval_metrics(y_test, preds_test)\n",
    "\n",
    "                model_results['xgb'] = {'cv_folds': folds, 'test_metrics': test_metrics, 'best_params': xgb_params}\n",
    "                save_dir = os.path.join(out_dir, t)\n",
    "                meta = save_artifacts(final_model, scaler, out_dir=save_dir, name=f'{t}_xgb', predictors=X_cols)\n",
    "                model_results['xgb']['saved'] = meta\n",
    "\n",
    "                # generate full predictions for backtesting if requested\n",
    "                if generate_full_preds:\n",
    "                    # if fast_mode: only produce preds for the holdout + last row to save time\n",
    "                    if fast_mode:\n",
    "                        # predict only the holdout (test) and the last row\n",
    "                        print(\"Fast mode: generating predictions for holdout portion only (faster)\")\n",
    "                        # predictions for each row in test set using model trained on train_all\n",
    "                        df_preds = pd.DataFrame()\n",
    "                        df_preds['Date'] = df['Date'].iloc[split_idx:].reset_index(drop=True) if 'Date' in df.columns else pd.RangeIndex(start=split_idx, stop=len(df))\n",
    "                        df_preds[f'pred_{t}'] = preds_test.tolist()\n",
    "                    else:\n",
    "                        print(\"Generating expanding-window one-step-ahead predictions (this may take time)...\")\n",
    "                        # Prepare df_features => must match the X_cols order and be cleaned of NaNs\n",
    "                        df_features = df[['Date'] + X_cols + [t]].copy()\n",
    "                        # Note: df has next_t as target aligned to same row; we will use X_cols and target column\n",
    "                        model_params = xgb_params.copy()\n",
    "                        # ensure initial_train_size reasonable\n",
    "                        if initial_train_size is None:\n",
    "                            initial_train_size = int(len(df_features) * 0.5)\n",
    "                        df_preds = generate_expanding_one_step_predictions(df_features[['Date'] + X_cols + [t]], X_cols, t, model_params, initial_train_size=initial_train_size)\n",
    "                    # save df_preds to models/<target> folder\n",
    "                    preds_out_dir = os.path.join(out_dir, t)\n",
    "                    os.makedirs(preds_out_dir, exist_ok=True)\n",
    "                    preds_csv = os.path.join(preds_out_dir, f'predictions_full_{t}.csv')\n",
    "                    df_preds.to_csv(preds_csv, index=False)\n",
    "                    print(f\"Saved full predictions to {preds_csv}\")\n",
    "\n",
    "        if model_type in ('rf', 'all'):\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            rf = RandomForestRegressor(n_estimators=200, random_state=SEED, n_jobs=-1)\n",
    "            rf.fit(X_train_all_s, y_train_all)\n",
    "            preds = rf.predict(X_test_s)\n",
    "            model_results['rf'] = {'test_metrics': eval_metrics(y_test, preds)}\n",
    "            meta = save_artifacts(rf, scaler, out_dir=os.path.join(out_dir, t), name=f'{t}_rf', predictors=X_cols)\n",
    "            model_results['rf']['saved'] = meta\n",
    "\n",
    "        summary[t] = model_results\n",
    "        print(f\"Finished target={t}. Summary:\")\n",
    "        print(json.dumps(model_results.get('xgb', model_results), indent=2, default=str))\n",
    "\n",
    "    # write summary\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    with open(os.path.join(out_dir, 'training_summary.json'), 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data', type=str, default='reliance.csv',\n",
    "                        help='Path to engineered features CSV (must contain Open and Close)')\n",
    "    parser.add_argument('--lags', type=int, nargs='+', default=[1, 2, 3, 5],\n",
    "                        help='Lag periods to generate (e.g. --lags 1 2 3 5)')\n",
    "    parser.add_argument('--model', type=str, default='xgb', choices=['xgb', 'rf', 'all'],\n",
    "                        help='Model to train (xgb recommended)')\n",
    "    parser.add_argument('--target', type=str, default='next_close', choices=['next_open', 'next_close', 'both'],\n",
    "                        help='Which target to train')\n",
    "    parser.add_argument('--test_size', type=float, default=0.2, help='Test set proportion (chronological)')\n",
    "    parser.add_argument('--preserve_time', action='store_true', help='Use chronological split (recommended)')\n",
    "    parser.add_argument('--out_dir', type=str, default='models', help='Output folder to save models and scalers')\n",
    "    parser.add_argument('--use_optuna', action='store_true', help='Enable Optuna tuning for XGBoost')\n",
    "    parser.add_argument('--n_trials', type=int, default=20, help='Number of Optuna trials')\n",
    "    parser.add_argument('--n_splits', type=int, default=5, help='Number of folds for walk-forward CV')\n",
    "    parser.add_argument('--initial_train_size', type=int, default=None,\n",
    "                        help='Initial train size (rows) for walk-forward CV (optional)')\n",
    "    parser.add_argument('--generate_full_preds', action='store_true', help='Generate expanding-window full prediction history for backtesting')\n",
    "    parser.add_argument('--fast_mode', action='store_true', help='Fast mode: only produce holdout predictions (not full history)')\n",
    "\n",
    "\n",
    "    # Notebook-safe\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    run_pipeline(\n",
    "        data_path = args.data,\n",
    "        lags = args.lags,\n",
    "        model_type = args.model,\n",
    "        target = args.target,\n",
    "        test_size = args.test_size,\n",
    "        preserve_time = args.preserve_time,\n",
    "        out_dir = args.out_dir,\n",
    "        use_optuna = args.use_optuna,\n",
    "        n_trials = args.n_trials,\n",
    "        n_splits = args.n_splits,\n",
    "        initial_train_size = args.initial_train_size,\n",
    "        generate_full_preds = args.generate_full_preds,\n",
    "        fast_mode = args.fast_mode\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e040b-2471-4594-9c54-39493dbaed88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b18e51-02ab-454e-bdf2-de7405ffcc9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc8dfe5-da19-49fa-ab4f-316090df3cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3930bc15-740b-4c4c-87e0-94e633270d76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2390199b-8d06-42cd-91ce-57d25c48c0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2157f605-ed7f-43b3-9fdb-acac5c4cf96f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea1558c-0d9f-4989-8efc-e3289387b72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715a4fb5-d94e-4b3c-bf45-aad8266cbee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9142fcb-af67-4280-9314-e81374ce3694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027e0cc-975d-41d0-b953-1c306f65f90a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69098d57-a4b9-4934-bd43-301ab66276ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be30ebdd-9b43-4f30-a21f-21448fc5e5fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c47c268-a263-4eb2-a8cb-9d01426597af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b28b68-1e91-48cd-a6f6-dd3a7960043b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bff36f-9903-488a-abd1-eca92acb2e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30ab448-b074-4c29-a5cc-2ae3a4f4556c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e9e2b0-1712-4be9-8b2d-cfae1b54e5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
