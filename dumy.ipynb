{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e162c7-1070-420e-8cdf-88cd6d48c9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf05bd5-b0af-4574-a5e7-e949c406b93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Tuple, List, Dict, Any\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from Feature_engineering import DEFAULT_FEATURES\n",
    "\n",
    "DEFAULT_XGB_PARAMS = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    n_estimators=500,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    ")\n",
    "\n",
    "\n",
    "def _compute_metric(y_true: pd.Series, y_pred: np.ndarray, metric: str) -> float:\n",
    "    if metric == \"accuracy\":\n",
    "        return float(accuracy_score(y_true, y_pred))\n",
    "    if metric == \"precision\":\n",
    "        return float(precision_score(y_true, y_pred, zero_division=0))\n",
    "    if metric == \"recall\":\n",
    "        return float(recall_score(y_true, y_pred, zero_division=0))\n",
    "    # default to f1\n",
    "    return float(f1_score(y_true, y_pred, zero_division=0))\n",
    "\n",
    "\n",
    "class PriceClassifier:\n",
    "    \"\"\"\n",
    "    PriceClassifier supporting time-series cross-validation.\n",
    "\n",
    "    Usage:\n",
    "      - clf = PriceClassifier()\n",
    "      - clf.fit(X, y)                      # single time-ordered split\n",
    "      - clf.fit_tscv(X, y, n_splits=5)     # picks best fold model from TimeSeriesSplit\n",
    "      - clf.predict(X_new)\n",
    "      - clf.save(\"model.pkl\"), PriceClassifier.load(\"model.pkl\")\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params: Optional[dict] = None, feature_columns: Optional[List[str]] = None):\n",
    "        self.params = (params or DEFAULT_XGB_PARAMS.copy()).copy()\n",
    "        if feature_columns is not None:\n",
    "            self.feature_columns = list(feature_columns)\n",
    "        elif DEFAULT_FEATURES is not None:\n",
    "            self.feature_columns = list(DEFAULT_FEATURES)\n",
    "        else:\n",
    "            self.feature_columns = None\n",
    "\n",
    "        self.model = XGBClassifier(**self.params)\n",
    "        self.pipeline = Pipeline([(\"scaler\", StandardScaler()), (\"model\", self.model)])\n",
    "        self.fitted = False\n",
    "        self.best_iteration_ = None\n",
    "        self._last_tscv_info: Optional[Dict[str, Any]] = None\n",
    "\n",
    "    def _ensure_features_present(self, X: pd.DataFrame):\n",
    "        if self.feature_columns is None:\n",
    "            raise ValueError(\"feature_columns not set. Provide at init or ensure DEFAULT_FEATURES is available.\")\n",
    "        missing = [c for c in self.feature_columns if c not in X.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required feature columns in input: {missing}\")\n",
    "\n",
    "    def _prepare_Xy(self, X: pd.DataFrame, y: pd.Series, time_col: Optional[str]) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"Sort by time_col if provided; otherwise return copies.\"\"\"\n",
    "        Xc = X.copy()\n",
    "        yc = y.copy()\n",
    "        if time_col is not None:\n",
    "            if time_col not in Xc.columns:\n",
    "                raise ValueError(f\"time_col '{time_col}' not found in X columns.\")\n",
    "            # sort by time_col ascending\n",
    "            Xc = Xc.sort_values(time_col).reset_index(drop=True)\n",
    "            # align y with Xc order if y has matching index values\n",
    "            if isinstance(yc, pd.Series) and not yc.index.equals(X.index):\n",
    "                # assume yc corresponds row-wise; reset yc to sequential to match Xc\n",
    "                yc = pd.Series(yc.values, index=Xc.index)\n",
    "            else:\n",
    "                yc = yc.reset_index(drop=True)\n",
    "        return Xc, yc\n",
    "\n",
    "    def fit(self,\n",
    "            X: pd.DataFrame,\n",
    "            y: pd.Series,\n",
    "            val_split: float = 0.2,\n",
    "            early_stopping_rounds: Optional[int] = 25,\n",
    "            time_col: Optional[str] = None) -> \"PriceClassifier\":\n",
    "        \"\"\"\n",
    "        Fit using a single time-ordered holdout (train on earliest portion, validate on most recent val_split).\n",
    "        If time_col is provided, rows are sorted by it before splitting.\n",
    "        \"\"\"\n",
    "        Xc, yc = self._prepare_Xy(X, y, time_col)\n",
    "\n",
    "        if self.feature_columns is None:\n",
    "            self.feature_columns = list(Xc.columns)\n",
    "\n",
    "        Xf = Xc[self.feature_columns]\n",
    "        ycopy = yc.copy()\n",
    "\n",
    "        X_tr, X_va, y_tr, y_va = train_test_split(Xf, ycopy, test_size=val_split, shuffle=False, random_state=42)\n",
    "\n",
    "        scaler = self.pipeline.named_steps[\"scaler\"]\n",
    "        scaler.fit(X_tr)\n",
    "        Xtr_scaled = scaler.transform(X_tr)\n",
    "        Xva_scaled = scaler.transform(X_va)\n",
    "\n",
    "        self.model.set_params(**self.params)\n",
    "        self.model.fit(\n",
    "            Xtr_scaled,\n",
    "            y_tr,\n",
    "            eval_set=[(Xva_scaled, y_va)] if early_stopping_rounds else None,\n",
    "            early_stopping_rounds=early_stopping_rounds,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        self.pipeline = Pipeline([(\"scaler\", scaler), (\"model\", self.model)])\n",
    "        self.fitted = True\n",
    "        if hasattr(self.model, \"best_iteration_\"):\n",
    "            try:\n",
    "                self.best_iteration_ = int(self.model.best_iteration_)\n",
    "            except Exception:\n",
    "                self.best_iteration_ = None\n",
    "        return self\n",
    "\n",
    "    def fit_tscv(self,\n",
    "                 X: pd.DataFrame,\n",
    "                 y: pd.Series,\n",
    "                 n_splits: int = 5,\n",
    "                 metric: str = \"f1\",\n",
    "                 early_stopping_rounds: Optional[int] = 25,\n",
    "                 time_col: Optional[str] = None,\n",
    "                 group_col: Optional[str] = None) -> \"PriceClassifier\":\n",
    "        \"\"\"\n",
    "        Fit using TimeSeriesSplit and keep the model from the fold with best validation metric.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X, y : data\n",
    "        n_splits : int\n",
    "            Number of folds for TimeSeriesSplit. Must be >=2.\n",
    "        metric : str\n",
    "            Metric to select best fold (\"accuracy\", \"precision\", \"recall\", \"f1\").\n",
    "        early_stopping_rounds : Optional[int]\n",
    "            Passed to xgboost fit for each fold.\n",
    "        time_col : Optional[str]\n",
    "            If provided, data is sorted by this column before splitting.\n",
    "        group_col : Optional[str]\n",
    "            If provided, performs group-wise fold assignment: entire groups are assigned to folds in chronological\n",
    "            order of group start times. This prevents leakage for panel data (multiple time series).\n",
    "        \"\"\"\n",
    "        if n_splits < 2:\n",
    "            raise ValueError(\"n_splits must be >= 2 for TimeSeriesSplit\")\n",
    "\n",
    "        # Prepare / sort\n",
    "        Xc, yc = self._prepare_Xy(X, y, time_col)\n",
    "\n",
    "        # Determine feature columns\n",
    "        if self.feature_columns is None:\n",
    "            self.feature_columns = list(Xc.columns)\n",
    "\n",
    "        Xf = Xc[self.feature_columns].reset_index(drop=True)\n",
    "        ycopy = yc.reset_index(drop=True)\n",
    "\n",
    "        # Build fold indices\n",
    "        if group_col is not None:\n",
    "            if group_col not in Xc.columns:\n",
    "                raise ValueError(f\"group_col '{group_col}' not found in X columns.\")\n",
    "            # Order groups by min time_col if provided else by group label order\n",
    "            if time_col is not None:\n",
    "                grp_order = (Xc.groupby(group_col)[time_col].min()\n",
    "                             .sort_values()\n",
    "                             .index\n",
    "                             .tolist())\n",
    "            else:\n",
    "                grp_order = Xc[group_col].drop_duplicates().tolist()\n",
    "\n",
    "            # Split groups into n_splits sequential buckets (approx equal by count of groups)\n",
    "            groups = grp_order\n",
    "            group_buckets = [[] for _ in range(n_splits)]\n",
    "            for i, g in enumerate(groups):\n",
    "                group_buckets[i % n_splits].append(g)\n",
    "\n",
    "            # Map group -> indices\n",
    "            group_to_indices = {g: sub.index.tolist() for g, sub in Xc.groupby(group_col)}\n",
    "\n",
    "            # Build fold index pairs (train_idx, val_idx)\n",
    "            fold_indices: List[Tuple[np.ndarray, np.ndarray]] = []\n",
    "            # For time-series behavior, use increasing training window: for i in range(1, n_splits)\n",
    "            for i in range(1, n_splits):\n",
    "                train_groups = [g for bucket in group_buckets[:i] for g in bucket]\n",
    "                val_groups = group_buckets[i]\n",
    "                train_idx = [idx for g in train_groups for idx in group_to_indices.get(g, [])]\n",
    "                val_idx = [idx for g in val_groups for idx in group_to_indices.get(g, [])]\n",
    "                if len(train_idx) == 0 or len(val_idx) == 0:\n",
    "                    continue\n",
    "                fold_indices.append((np.array(sorted(train_idx)), np.array(sorted(val_idx))))\n",
    "        else:\n",
    "            tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "            fold_indices = list(tscv.split(Xf))\n",
    "\n",
    "        # Iterate folds, train & evaluate, keep best model\n",
    "        best_score = -np.inf\n",
    "        best_model = None\n",
    "        best_scaler = None\n",
    "        best_fold_info = None\n",
    "\n",
    "        for fold_num, (train_idx, val_idx) in enumerate(fold_indices):\n",
    "            X_train, X_val = Xf.iloc[train_idx], Xf.iloc[val_idx]\n",
    "            y_train, y_val = ycopy.iloc[train_idx], ycopy.iloc[val_idx]\n",
    "\n",
    "            # Fit scaler on X_train\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(X_train)\n",
    "            Xtr_scaled = scaler.transform(X_train)\n",
    "            Xva_scaled = scaler.transform(X_val)\n",
    "\n",
    "            # Train xgboost on this fold\n",
    "            model = XGBClassifier(**self.params)\n",
    "            model.fit(\n",
    "                Xtr_scaled,\n",
    "                y_train,\n",
    "                eval_set=[(Xva_scaled, y_val)] if early_stopping_rounds else None,\n",
    "                early_stopping_rounds=early_stopping_rounds,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            # Evaluate on validation fold using chosen metric (threshold 0.5)\n",
    "            y_val_pred = (model.predict_proba(Xva_scaled)[:, 1] >= 0.5).astype(int)\n",
    "            score = _compute_metric(y_val, y_val_pred, metric)\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_model = model\n",
    "                best_scaler = scaler\n",
    "                best_fold_info = {\n",
    "                    \"fold\": fold_num,\n",
    "                    \"train_size\": int(len(train_idx)),\n",
    "                    \"val_size\": int(len(val_idx)),\n",
    "                    \"metric\": metric,\n",
    "                    \"score\": float(score),\n",
    "                }\n",
    "\n",
    "        if best_model is None:\n",
    "            raise RuntimeError(\"No valid folds produced a trained model. Check n_splits and your data.\")\n",
    "\n",
    "        # Set best model and scaler as the official model\n",
    "        self.model = best_model\n",
    "        self.pipeline = Pipeline([(\"scaler\", best_scaler), (\"model\", self.model)])\n",
    "        self.fitted = True\n",
    "        if hasattr(self.model, \"best_iteration_\"):\n",
    "            try:\n",
    "                self.best_iteration_ = int(self.model.best_iteration_)\n",
    "            except Exception:\n",
    "                self.best_iteration_ = None\n",
    "\n",
    "        self._last_tscv_info = best_fold_info\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        if not self.fitted:\n",
    "            raise AssertionError(\"Model not fitted. Call .fit() or .fit_tscv() first or .load() a saved model.\")\n",
    "        self._ensure_features_present(X)\n",
    "        Xs = X[self.feature_columns]\n",
    "        scaler = self.pipeline.named_steps[\"scaler\"]\n",
    "        Xs_scaled = scaler.transform(Xs)\n",
    "        proba = self.model.predict_proba(Xs_scaled)\n",
    "        return proba[:, 1] if proba.ndim > 1 else proba\n",
    "\n",
    "    def predict(self, X: pd.DataFrame, threshold: float = 0.55) -> np.ndarray:\n",
    "        p = self.predict_proba(X)\n",
    "        return (p >= threshold).astype(int)\n",
    "\n",
    "    def evaluate(self, X: pd.DataFrame, y_true: pd.Series, threshold: float = 0.55) -> Dict[str, Any]:\n",
    "        y_pred = self.predict(X, threshold=threshold)\n",
    "        return {\n",
    "            \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "            \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "            \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "            \"f1\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "            \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist(),\n",
    "        }\n",
    "\n",
    "    def save(self, path: str) -> None:\n",
    "        payload = {\n",
    "            \"params\": self.params,\n",
    "            \"feature_columns\": self.feature_columns,\n",
    "            \"model\": self.model,\n",
    "            \"scaler\": self.pipeline.named_steps[\"scaler\"],\n",
    "            \"best_iteration_\": self.best_iteration_,\n",
    "        }\n",
    "        joblib.dump(payload, path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> \"PriceClassifier\":\n",
    "        obj = joblib.load(path)\n",
    "        pc = cls(params=obj.get(\"params\"), feature_columns=obj.get(\"feature_columns\"))\n",
    "        pc.model = obj[\"model\"]\n",
    "        saved_scaler = obj.get(\"scaler\", StandardScaler())\n",
    "        pc.pipeline = Pipeline([(\"scaler\", saved_scaler), (\"model\", pc.model)])\n",
    "        pc.fitted = True\n",
    "        pc.best_iteration_ = obj.get(\"best_iteration_\", None)\n",
    "        return pc\n",
    "\n",
    "    def classification_report(self, y_true: pd.Series, y_pred: np.ndarray) -> dict:\n",
    "        return {\n",
    "            \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "            \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "            \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "            \"f1\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "            \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd918fd-bdc0-4627-8dbe-5d1fa3aebeda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f5d2f-bf17-478f-8e46-41eb8145b865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74145b7f-137d-49e7-8048-83a2064bb7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd9a8fc-68b1-4eeb-b5d4-f4bdeddcb2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    mean_absolute_error, mean_squared_error, r2_score\n",
    ")\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "# -------------------------\n",
    "# Utility: direction accuracy for regression (optional)\n",
    "# -------------------------\n",
    "def direction_accuracy(y_true, y_pred):\n",
    "    \"\"\"Fraction of samples where sign(pred) == sign(true).\"\"\"\n",
    "    s_true = np.sign(y_true)\n",
    "    s_pred = np.sign(y_pred)\n",
    "    return np.mean(s_true == s_pred)\n",
    "\n",
    "# -------------------------\n",
    "# Classification: time-aware trainer\n",
    "# -------------------------\n",
    "def train_classification_timeaware(\n",
    "    X, y_cls,\n",
    "    save_dir=\"models\",\n",
    "    test_size=0.2,\n",
    "    use_timeseries_cv=True,\n",
    "    n_splits=5,\n",
    "    cls_params=None,\n",
    "    random_state=42\n",
    "):\n",
    "    if cls_params is None:\n",
    "        cls_params = {\n",
    "            \"n_estimators\": 300,\n",
    "            \"max_depth\": 4,\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"verbosity\": 0,\n",
    "            \"random_state\": random_state,\n",
    "            \"use_label_encoder\": False,\n",
    "            \"eval_metric\": \"logloss\",\n",
    "        }\n",
    "\n",
    "    X_df = X.copy() if not isinstance(X, np.ndarray) else pd.DataFrame(X)\n",
    "    y = pd.Series(y_cls).copy()\n",
    "    mask = ~y.isna()\n",
    "    X_df = X_df.loc[mask]\n",
    "    y = y.loc[mask]\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    if use_timeseries_cv:\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        fold_metrics = []\n",
    "        last_model = None\n",
    "        last_scaler = None\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(tscv.split(X_df)):\n",
    "            X_train, X_val = X_df.iloc[train_idx], X_df.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            X_train_s = scaler.fit_transform(X_train)\n",
    "            X_val_s = scaler.transform(X_val)\n",
    "\n",
    "            clf = XGBClassifier(**cls_params)\n",
    "            clf.fit(X_train_s, y_train, eval_set=[(X_val_s, y_val)],\n",
    "                    early_stopping_rounds=25, verbose=False)\n",
    "\n",
    "            preds = clf.predict(X_val_s)\n",
    "\n",
    "            m = {\n",
    "                \"accuracy\": accuracy_score(y_val, preds),\n",
    "                \"precision\": precision_score(y_val, preds, zero_division=0),\n",
    "                \"recall\": recall_score(y_val, preds, zero_division=0),\n",
    "                \"f1\": f1_score(y_val, preds, zero_division=0)\n",
    "            }\n",
    "            fold_metrics.append(m)\n",
    "\n",
    "            last_model = clf\n",
    "            last_scaler = scaler\n",
    "\n",
    "        avg_metrics = {k: np.mean([fm[k] for fm in fold_metrics]) for k in fold_metrics[0]}\n",
    "        joblib.dump(last_model, os.path.join(save_dir, \"trained_classifier.pkl\"))\n",
    "        joblib.dump(last_scaler, os.path.join(save_dir, \"classifier_scaler.pkl\"))\n",
    "\n",
    "        print(\"Classification (TimeSeriesSplit) complete. Avg metrics across folds:\")\n",
    "        for k, v in avg_metrics.items():\n",
    "            print(f\"{k}: {v:.6f}\")\n",
    "        print(f\"Saved classifier -> {os.path.join(save_dir, 'trained_classifier.pkl')}\")\n",
    "\n",
    "        return last_model, last_scaler, avg_metrics\n",
    "\n",
    "    else:\n",
    "        n = len(X_df)\n",
    "        test_n = int(n * test_size)\n",
    "        train_n = n - test_n\n",
    "\n",
    "        X_train = X_df.iloc[:train_n]\n",
    "        X_test = X_df.iloc[train_n:]\n",
    "        y_train = y.iloc[:train_n]\n",
    "        y_test = y.iloc[train_n:]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_s = scaler.fit_transform(X_train)\n",
    "        X_test_s = scaler.transform(X_test)\n",
    "\n",
    "        clf = XGBClassifier(**cls_params)\n",
    "        clf.fit(X_train_s, y_train, eval_set=[(X_test_s, y_test)],\n",
    "                early_stopping_rounds=25, verbose=False)\n",
    "\n",
    "        preds = clf.predict(X_test_s)\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_test, preds),\n",
    "            \"precision\": precision_score(y_test, preds, zero_division=0),\n",
    "            \"recall\": recall_score(y_test, preds, zero_division=0),\n",
    "            \"f1\": f1_score(y_test, preds, zero_division=0)\n",
    "        }\n",
    "\n",
    "        joblib.dump(clf, os.path.join(save_dir, \"trained_classifier.pkl\"))\n",
    "        joblib.dump(scaler, os.path.join(save_dir, \"classifier_scaler.pkl\"))\n",
    "\n",
    "        print(\"Classification (chronological split) complete. Metrics (test set):\")\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"{k}: {v:.6f}\")\n",
    "        print(f\"Saved classifier -> {os.path.join(save_dir, 'trained_classifier.pkl')}\")\n",
    "\n",
    "        return clf, scaler, metrics\n",
    "\n",
    "# -------------------------\n",
    "# Regression: time-aware trainer\n",
    "# -------------------------\n",
    "def train_regression_timeaware(\n",
    "    X, y_reg,\n",
    "    save_dir=\"models\",\n",
    "    test_size=0.2,\n",
    "    use_timeseries_cv=True,\n",
    "    n_splits=5,\n",
    "    xgb_params=None,\n",
    "    random_state=42\n",
    "):\n",
    "    if xgb_params is None:\n",
    "        xgb_params = {\n",
    "            \"n_estimators\": 500,\n",
    "            \"max_depth\": 4,\n",
    "            \"learning_rate\": 0.03,\n",
    "            \"verbosity\": 0,\n",
    "            \"random_state\": random_state,\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "        }\n",
    "\n",
    "    X_df = X.copy() if not isinstance(X, np.ndarray) else pd.DataFrame(X)\n",
    "    y = pd.Series(y_reg).copy()\n",
    "    mask = ~y.isna()\n",
    "    X_df = X_df.loc[mask]\n",
    "    y = y.loc[mask]\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    if use_timeseries_cv:\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        val_metrics = []\n",
    "        last_model = None\n",
    "        last_scaler = None\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(tscv.split(X_df)):\n",
    "            X_train, X_val = X_df.iloc[train_idx], X_df.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            X_train_s = scaler.fit_transform(X_train)\n",
    "            X_val_s = scaler.transform(X_val)\n",
    "\n",
    "            model = XGBRegressor(**xgb_params)\n",
    "            model.fit(X_train_s, y_train, eval_set=[(X_val_s, y_val)],\n",
    "                      early_stopping_rounds=25, verbose=False)\n",
    "\n",
    "            preds = model.predict(X_val_s)\n",
    "            mae = mean_absolute_error(y_val, preds)\n",
    "            mse = mean_squared_error(y_val, preds)\n",
    "            rmse = np.sqrt(mse)\n",
    "            r2 = r2_score(y_val, preds)\n",
    "            dir_acc = direction_accuracy(y_val.values, preds)\n",
    "\n",
    "            val_metrics.append({\"mae\": mae, \"mse\": mse, \"rmse\": rmse, \"r2\": r2, \"dir_acc\": dir_acc})\n",
    "\n",
    "            last_model = model\n",
    "            last_scaler = scaler\n",
    "\n",
    "        avg_metrics = {k: np.mean([m[k] for m in val_metrics]) for k in val_metrics[0]}\n",
    "\n",
    "        joblib.dump(last_model, os.path.join(save_dir, \"trained_regressor.pkl\"))\n",
    "        joblib.dump(last_scaler, os.path.join(save_dir, \"regressor_scaler.pkl\"))\n",
    "\n",
    "        print(\"Regression (TimeSeriesSplit) complete. Avg metrics across folds:\")\n",
    "        for k, v in avg_metrics.items():\n",
    "            print(f\"{k}: {v:.6f}\")\n",
    "        print(f\"Saved regressor -> {os.path.join(save_dir, 'trained_regressor.pkl')}\")\n",
    "\n",
    "        return last_model, last_scaler, avg_metrics\n",
    "\n",
    "    else:\n",
    "        n = len(X_df)\n",
    "        test_n = int(n * test_size)\n",
    "        train_n = n - test_n\n",
    "\n",
    "        X_train = X_df.iloc[:train_n]\n",
    "        X_test = X_df.iloc[train_n:]\n",
    "        y_train = y.iloc[:train_n]\n",
    "        y_test = y.iloc[train_n:]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_s = scaler.fit_transform(X_train)\n",
    "        X_test_s = scaler.transform(X_test)\n",
    "\n",
    "        model = XGBRegressor(**xgb_params)\n",
    "        model.fit(X_train_s, y_train, eval_set=[(X_test_s, y_test)],\n",
    "                  early_stopping_rounds=25, verbose=False)\n",
    "\n",
    "        preds = model.predict(X_test_s)\n",
    "        mae = mean_absolute_error(y_test, preds)\n",
    "        mse = mean_squared_error(y_test, preds)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, preds)\n",
    "        dir_acc = direction_accuracy(y_test.values, preds)\n",
    "\n",
    "        metrics = {\"mae\": mae, \"mse\": mse, \"rmse\": rmse, \"r2\": r2, \"dir_acc\": dir_acc}\n",
    "\n",
    "        joblib.dump(model, os.path.join(save_dir, \"trained_regressor.pkl\"))\n",
    "        joblib.dump(scaler, os.path.join(save_dir, \"regressor_scaler.pkl\"))\n",
    "\n",
    "        print(\"Regression (chronological split) complete. Metrics (test set):\")\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"{k}: {v:.6f}\")\n",
    "        print(f\"Saved regressor -> {os.path.join(save_dir, 'trained_regressor.pkl')}\")\n",
    "\n",
    "        return model, scaler, metrics\n",
    "\n",
    "# -------------------------\n",
    "# Convenience: train both models\n",
    "# -------------------------\n",
    "def train_both(\n",
    "    X, y_cls, y_reg,\n",
    "    save_dir=\"models\",\n",
    "    cls_use_timeseries_cv=True,\n",
    "    reg_use_timeseries_cv=True,\n",
    "    n_splits=5\n",
    "):\n",
    "    out = {}\n",
    "    print(\"=== Training Classification ===\")\n",
    "    clf, scaler_clf, clf_metrics = train_classification_timeaware(\n",
    "        X, y_cls, save_dir=save_dir,\n",
    "        use_timeseries_cv=cls_use_timeseries_cv, n_splits=n_splits\n",
    "    )\n",
    "    out[\"classifier\"] = {\"model\": clf, \"scaler\": scaler_clf, \"metrics\": clf_metrics}\n",
    "\n",
    "    print(\"\\n=== Training Regression ===\")\n",
    "    reg, scaler_reg, reg_metrics = train_regression_timeaware(\n",
    "        X, y_reg, save_dir=save_dir,\n",
    "        use_timeseries_cv=reg_use_timeseries_cv, n_splits=n_splits\n",
    "    )\n",
    "    out[\"regressor\"] = {\"model\": reg, \"scaler\": scaler_reg, \"metrics\": reg_metrics}\n",
    "\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Prediction helpers\n",
    "# -------------------------\n",
    "def predict_regression(X_new, scaler_path=\"models/regressor_scaler.pkl\", model_path=\"models/trained_regressor.pkl\"):\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    model = joblib.load(model_path)\n",
    "    Xs = scaler.transform(X_new)\n",
    "    preds = model.predict(Xs)\n",
    "    return preds\n",
    "\n",
    "def predict_classification(X_new, scaler_path=\"models/classifier_scaler.pkl\", model_path=\"models/trained_classifier.pkl\"):\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    model = joblib.load(model_path)\n",
    "    Xs = scaler.transform(X_new)\n",
    "    preds = model.predict(Xs)\n",
    "    return preds\n",
    "\n",
    "# -------------------------\n",
    "# Main usage message (no automatic download or demo)\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"model.py loaded. This module exposes:\")\n",
    "    print(\" - train_classification_timeaware(X, y_cls, ...)\")\n",
    "    print(\" - train_regression_timeaware(X, y_reg, ...)\")\n",
    "    print(\" - train_both(X, y_cls, y_reg, ...)\")\n",
    "    print(\" - predict_classification(X_new, ...)\")\n",
    "    print(\" - predict_regression(X_new, ...)\")\n",
    "    print(\"\\nRun these from a notebook where you prepare X, y_cls, y_reg (no automatic data download).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37df52b-4237-4418-be5e-30e65ec68e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Feature_engineering import FeatureBuilder  # adjust import if filename differs\n",
    "\n",
    "# -------------- load CSV and prepare ----------------\n",
    "df = pd.read_csv(\"RELIANCE.csv\", parse_dates=[\"Date\"], dayfirst=False)  # change filename if needed\n",
    "print(\"Loaded shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# If 'Date' column exists, set it as index so it's not part of numeric columns\n",
    "if \"Date\" in df.columns:\n",
    "    df = df.set_index(\"Date\")\n",
    "    print(\"Set 'Date' as index. Index dtype:\", df.index.dtype)\n",
    "\n",
    "# Identify non-numeric columns (these would break .astype(float))\n",
    "non_numeric = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "if non_numeric:\n",
    "    print(\"Non-numeric columns found (they will be dropped):\", non_numeric)\n",
    "    df = df.drop(columns=non_numeric)\n",
    "\n",
    "# Quick check for common required columns\n",
    "required_cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"]\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    print(\"WARNING â€” missing these expected columns:\", missing)\n",
    "    print(\"Your FeatureBuilder may expect these exact names. Rename or create them if needed.\")\n",
    "else:\n",
    "    print(\"All expected OHLCV columns present.\")\n",
    "\n",
    "# Optionally show first rows\n",
    "print(df.head())\n",
    "\n",
    "# -------------- build dataset ----------------\n",
    "fb = FeatureBuilder()\n",
    "# FeatureBuilder expects a DataFrame without datetime columns; index can be datetime (good)\n",
    "X, y_cls, y_reg, features, aligned_df = fb.build_dataset(df)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y_cls shape:\", None if y_cls is None else y_cls.shape)\n",
    "print(\"y_reg shape:\", None if y_reg is None else y_reg.shape)\n",
    "print(\"Features used:\", features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f8393b-8fa3-4d0a-8ce6-e6a8cd0420c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f3c438-9a89-4eac-aab3-3c667d7f48ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0779375b-ccad-40fb-9674-01d2cc0ba7d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325b0d3-fc4a-4cfb-9bbd-d3a0613d34e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140828aa-a3c9-4d01-9ea1-4602db3ef2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de9c553-3ee4-43f3-b4c8-146d599e1615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d55bd49-d9e0-4ddc-ad90-3f4a269b0825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2ce675-a3b1-4ab2-af07-095c969a6952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d29770e-3adc-44bc-b2b6-c2af252f46f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969ac651-d227-4be0-95b1-ef24ab7c0b30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f9d23a-4ee4-4de9-9bef-22493cfe60dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c85d2d-ab23-4245-b95a-4d01924ef33c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9856f09b-bfa0-4b50-89c4-fd6f76e27893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a481c31-ee6c-4f01-b881-21effd0df393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12184930-3f3a-4faf-8a11-00097fdb4c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2999a465-fd19-493d-96a2-9a52c56c3614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448b0316-c2ad-4044-9aaa-a0a6f7755c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a914864a-b0e0-4489-9347-b722576abe18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4df8a4-eec2-4bab-84ec-564680bd7ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b77d76d-ec2a-4eb2-b418-06a77be95ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae45b3f-005a-40aa-abda-8469c10f53c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9ce3e1-e0ce-4aa2-bc61-6795114db742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d15af6f-8814-4f96-ae96-2bddf39a7b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2686f0-0be3-4779-9842-be2ecd3a70f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9adb5e3-d43f-4823-b98a-52c7445ff783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52976abc-ed69-4090-a059-78c123908617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0498c623-33dd-4b65-a0ae-567e5bb104a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f7151d-b3c3-4b6c-9323-c18e7cdeef70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d0e37-14bf-44d2-a9bc-9b2953063abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4cc168-9453-48f6-b76f-00fd298aed9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a695a31-bc21-44a6-bd2f-4408dbfdfe26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63b54d4-d177-48c1-af4c-4d48f9d1cb52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67a3eb3-f11c-45fe-8154-8db999e5656d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f6d5a3-49a5-4ecc-ab8f-72c0667d7005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e16aca-c833-44be-896c-f2bc5618937c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd82021c-2da8-4ed1-87c6-cf1737a52f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2609327d-8511-4818-8a45-4f73a7ff1f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3e5e0c-339b-4d1b-a4a1-b949dade5242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0f9cb4-f18d-4a18-a151-96aa099efab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9112e03a-a52c-4937-9758-ee8476162d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a0abb0-b7f5-47ba-9bf2-4ee1c1aae77c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7d3d67-f223-463f-91fa-881b167be304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176a7880-a546-4e82-a16e-813d0ad9f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
