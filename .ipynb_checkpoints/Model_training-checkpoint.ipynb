{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b448cf-3b7e-4f82-acc9-f4ae5bfed447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "#              Model_Training\n",
    "# -------------------------------------------------\n",
    "\n",
    "\"\"\" \n",
    "Reusable model class for daily stock prediction.\n",
    "            • Standardizes features with StandardScaler\n",
    "            • XGBoost classifier (binary up/down)\n",
    "            • Time-aware train/val split (no shuffling)\n",
    "            • Save/load with joblib\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa745eff-cb8f-4aa2-a1dc-63d92b2e6fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished target=next_close. Summary:\n",
      "{\n",
      "  \"cv_folds\": [\n",
      "    {\n",
      "      \"fold\": 0,\n",
      "      \"metrics\": {\n",
      "        \"mae\": 10.403113114206414,\n",
      "        \"rmse\": 13.011159313989314,\n",
      "        \"r2\": 0.9204854361254439,\n",
      "        \"mape\": 0.8674856048678252\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"fold\": 1,\n",
      "      \"metrics\": {\n",
      "        \"mae\": 95.48250379060444,\n",
      "        \"rmse\": 132.48770026458632,\n",
      "        \"r2\": -0.5170183556622374,\n",
      "        \"mape\": 6.660437696929019\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"fold\": 2,\n",
      "      \"metrics\": {\n",
      "        \"mae\": 21.839400442023027,\n",
      "        \"rmse\": 30.410088146436358,\n",
      "        \"r2\": 0.12059792776472666,\n",
      "        \"mape\": 1.4949401405752423\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"fold\": 3,\n",
      "      \"metrics\": {\n",
      "        \"mae\": 30.703370746813324,\n",
      "        \"rmse\": 41.514130605709205,\n",
      "        \"r2\": 0.5604184813886672,\n",
      "        \"mape\": 2.026895528753216\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"fold\": 4,\n",
      "      \"metrics\": {\n",
      "        \"mae\": 18.20243273283306,\n",
      "        \"rmse\": 23.21874105771824,\n",
      "        \"r2\": 0.6445653541759144,\n",
      "        \"mape\": 1.4185014042879882\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"test_metrics\": {\n",
      "    \"mae\": 13.699415989925987,\n",
      "    \"rmse\": 17.534786517412204,\n",
      "    \"r2\": 0.9653204869709503,\n",
      "    \"mape\": 0.9918648718693173\n",
      "  },\n",
      "  \"best_params\": {\n",
      "    \"n_estimators\": 500,\n",
      "    \"learning_rate\": 0.05,\n",
      "    \"max_depth\": 6,\n",
      "    \"subsample\": 0.8,\n",
      "    \"colsample_bytree\": 0.8,\n",
      "    \"random_state\": 42\n",
      "  },\n",
      "  \"saved\": {\n",
      "    \"model_path\": \"models\\\\next_close\\\\next_close_xgb_xgb.json\",\n",
      "    \"framework\": \"xgboost\",\n",
      "    \"scaler_path\": \"models\\\\next_close\\\\next_close_xgb_scaler.joblib\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "model.py - Final production-ready script for next-day Open/Close forecasting.\n",
    "\n",
    "Features:\n",
    " - Loads engineered features CSV (expects 'Open' and 'Close' columns)\n",
    " - Creates next-day targets: next_open, next_close\n",
    " - Adds configurable lag features\n",
    " - Standard scaling\n",
    " - Walk-forward (expanding-window) cross-validation\n",
    " - XGBoost training (recommended) with a robust fit call (works across xgboost versions)\n",
    " - Optional Optuna hyperparameter tuning for XGBoost\n",
    " - RandomForest baseline option\n",
    " - Saves model artifacts and training summary\n",
    " - Works inside Jupyter notebooks via parse_known_args()\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from typing import List, Optional, Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# Optional libraries\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except Exception:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "    OPTUNA_AVAILABLE = True\n",
    "except Exception:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "# ----------------- utilities -----------------\n",
    "\n",
    "\n",
    "def load_features(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    if 'Date' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df = df.sort_values('Date').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_next_day_targets(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if 'Open' not in df.columns or 'Close' not in df.columns:\n",
    "        raise ValueError(\"Input features must contain 'Open' and 'Close' columns.\")\n",
    "    df['next_open'] = df['Open'].shift(-1)\n",
    "    df['next_close'] = df['Close'].shift(-1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_lag_features(df: pd.DataFrame, cols: List[str], lags: List[int]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        for l in lags:\n",
    "            df[f\"{c}_lag_{l}\"] = df[c].shift(l)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_predictor_columns(df: pd.DataFrame, exclude: List[str]) -> List[str]:\n",
    "    exclude_set = set(exclude)\n",
    "    return [c for c in df.select_dtypes(include=[np.number]).columns if c not in exclude_set]\n",
    "\n",
    "\n",
    "def eval_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "    # Manual RMSE (compatible across sklearn versions)\n",
    "    mse = np.mean((np.array(y_true) - np.array(y_pred)) ** 2)\n",
    "    rmse = float(np.sqrt(mse))\n",
    "\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    # Safe MAPE\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        mape = np.mean(\n",
    "            np.abs((y_true - y_pred) / np.where(y_true == 0, 1e-8, y_true))\n",
    "        ) * 100\n",
    "\n",
    "    return {'mae': float(mae), 'rmse': rmse, 'r2': float(r2), 'mape': float(mape)}\n",
    "\n",
    "\n",
    "# ----------------- XGBoost training -----------------\n",
    "\n",
    "\n",
    "def train_xgb(X_train: np.ndarray, y_train: np.ndarray,\n",
    "              X_val: Optional[np.ndarray] = None, y_val: Optional[np.ndarray] = None,\n",
    "              params: Optional[Dict[str, Any]] = None) -> Any:\n",
    "    \"\"\"\n",
    "    Train XGBRegressor in a robust way that tolerates different xgboost sklearn-wrapper versions.\n",
    "    Returns the trained model (XGBRegressor).\n",
    "    \"\"\"\n",
    "    if not XGB_AVAILABLE:\n",
    "        raise RuntimeError('xgboost is not available. Install xgboost to use this trainer.')\n",
    "\n",
    "    params = params or {\n",
    "        'n_estimators': 500,\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': 6,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'random_state': SEED\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "\n",
    "    if X_val is not None and y_val is not None:\n",
    "        # Try common API with early_stopping_rounds, fall back to more compatible calls.\n",
    "        try:\n",
    "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=20, verbose=False)\n",
    "        except TypeError:\n",
    "            try:\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "            except TypeError:\n",
    "                model.fit(X_train, y_train)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ----------------- Walk-forward CV -----------------\n",
    "\n",
    "\n",
    "def walk_forward_split(n_samples: int, n_splits: int = 5, initial_train_size: Optional[int] = None) -> List[Tuple[slice, slice]]:\n",
    "    \"\"\"\n",
    "    Return list of (train_slice, test_slice) for expanding-window CV.\n",
    "    \"\"\"\n",
    "    if initial_train_size is None:\n",
    "        initial_train_size = int(n_samples * 0.5)\n",
    "    test_size = int((n_samples - initial_train_size) / n_splits)\n",
    "    if test_size < 1:\n",
    "        test_size = 1\n",
    "    splits = []\n",
    "    train_start = 0\n",
    "    train_end = initial_train_size\n",
    "    for i in range(n_splits):\n",
    "        test_start = train_end\n",
    "        test_end = min(test_start + test_size, n_samples)\n",
    "        if test_start >= n_samples:\n",
    "            break\n",
    "        splits.append((slice(train_start, train_end), slice(test_start, test_end)))\n",
    "        train_end = test_end\n",
    "        if test_end == n_samples:\n",
    "            break\n",
    "    return splits\n",
    "\n",
    "\n",
    "def run_walk_forward_cv(X: np.ndarray, y: np.ndarray, n_splits: int = 5, initial_train_size: Optional[int] = None,\n",
    "                        params: Optional[Dict[str, Any]] = None) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
    "    splits = walk_forward_split(len(X), n_splits=n_splits, initial_train_size=initial_train_size)\n",
    "    fold_results = []\n",
    "\n",
    "    for i, (train_slice, test_slice) in enumerate(splits):\n",
    "        X_tr, y_tr = X[train_slice], y[train_slice]\n",
    "        X_te, y_te = X[test_slice], y[test_slice]\n",
    "        model = train_xgb(X_tr, y_tr, X_val=X_te, y_val=y_te, params=params)\n",
    "        preds = model.predict(X_te)\n",
    "        metrics = eval_metrics(y_te, preds)\n",
    "        fold_results.append({'fold': i, 'metrics': metrics})\n",
    "    return fold_results, (params or {})\n",
    "\n",
    "\n",
    "# ----------------- Optuna objective -----------------\n",
    "\n",
    "\n",
    "def optuna_objective(trial, X: np.ndarray, y: np.ndarray, n_splits: int = 3, initial_train_size: Optional[int] = None) -> float:\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 10.0, log=True),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'random_state': SEED,\n",
    "    }\n",
    "\n",
    "    splits = walk_forward_split(len(X), n_splits=n_splits, initial_train_size=initial_train_size)\n",
    "    val_scores = []\n",
    "    for train_slice, test_slice in splits:\n",
    "        X_tr, y_tr = X[train_slice], y[train_slice]\n",
    "        X_te, y_te = X[test_slice], y[test_slice]\n",
    "        # For objective we can use the sklearn wrapper; handle early stopping similarly\n",
    "        try:\n",
    "            model = xgb.XGBRegressor(**params)\n",
    "            model.fit(X_tr, y_tr, eval_set=[(X_te, y_te)], early_stopping_rounds=20, verbose=False)\n",
    "        except Exception:\n",
    "            model = xgb.XGBRegressor(**params)\n",
    "            model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_te)\n",
    "        metrics = eval_metrics(y_te, preds)\n",
    "        val_scores.append(metrics['rmse'])\n",
    "    return float(np.mean(val_scores))\n",
    "\n",
    "\n",
    "# ----------------- persistence -----------------\n",
    "\n",
    "\n",
    "def save_artifacts(model: Any, scaler: Optional[StandardScaler], out_dir: str, name: str) -> Dict[str, Any]:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    meta: Dict[str, Any] = {}\n",
    "    try:\n",
    "        if hasattr(model, 'get_booster'):\n",
    "            # save booster for robustness\n",
    "            model.get_booster().save_model(os.path.join(out_dir, f\"{name}_xgb.json\"))\n",
    "            meta['model_path'] = os.path.join(out_dir, f\"{name}_xgb.json\")\n",
    "            meta['framework'] = 'xgboost'\n",
    "        else:\n",
    "            joblib.dump(model, os.path.join(out_dir, f\"{name}_model.joblib\"))\n",
    "            meta['model_path'] = os.path.join(out_dir, f\"{name}_model.joblib\")\n",
    "            meta['framework'] = 'joblib'\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to save model: {e}\")\n",
    "\n",
    "    if scaler is not None:\n",
    "        scaler_path = os.path.join(out_dir, f\"{name}_scaler.joblib\")\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "        meta['scaler_path'] = scaler_path\n",
    "\n",
    "    return meta\n",
    "\n",
    "\n",
    "# ----------------- main pipeline -----------------\n",
    "\n",
    "\n",
    "def run_pipeline(\n",
    "    data_path: str,\n",
    "    lags: List[int],\n",
    "    model_type: str,\n",
    "    target: str,\n",
    "    test_size: float,\n",
    "    preserve_time: bool,\n",
    "    out_dir: str,\n",
    "    use_optuna: bool = False,\n",
    "    n_trials: int = 20,\n",
    "    n_splits: int = 5,\n",
    "    initial_train_size: Optional[int] = None\n",
    ") -> Dict[str, Any]:\n",
    "    df = load_features(data_path)\n",
    "    df = create_next_day_targets(df)\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    exclude = ['next_open', 'next_close']\n",
    "    base_cols = [c for c in ['Open', 'Close'] if c in numeric_cols]\n",
    "    other_cols = [c for c in numeric_cols if c not in base_cols + exclude]\n",
    "    lag_cols = base_cols + other_cols\n",
    "\n",
    "    df = add_lag_features(df, lag_cols, lags)\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "    targets: List[str] = []\n",
    "    if target == 'next_open':\n",
    "        targets = ['next_open']\n",
    "    elif target == 'next_close':\n",
    "        targets = ['next_close']\n",
    "    elif target == 'both':\n",
    "        targets = ['next_open', 'next_close']\n",
    "\n",
    "    summary: Dict[str, Any] = {}\n",
    "\n",
    "    for t in targets:\n",
    "        X_cols = get_predictor_columns(df, exclude=[t])\n",
    "        X = df[X_cols].values\n",
    "        y = df[t].values\n",
    "\n",
    "        # final chronological split for holdout\n",
    "        split_idx = int(len(X) * (1 - test_size))\n",
    "        if preserve_time:\n",
    "            X_train_all, X_test = X[:split_idx], X[split_idx:]\n",
    "            y_train_all, y_test = y[:split_idx], y[split_idx:]\n",
    "        else:\n",
    "            # keep chronological split even if preserve_time is False for deterministic behaviour\n",
    "            X_train_all, X_test = X[:split_idx], X[split_idx:]\n",
    "            y_train_all, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_all_s = scaler.fit_transform(X_train_all)\n",
    "        X_test_s = scaler.transform(X_test)\n",
    "\n",
    "        model_results: Dict[str, Any] = {}\n",
    "\n",
    "        if model_type in ('xgb', 'all'):\n",
    "            if not XGB_AVAILABLE:\n",
    "                print('xgboost not installed — skipping xgb training')\n",
    "            else:\n",
    "                best_params: Optional[Dict[str, Any]] = None\n",
    "                if use_optuna and OPTUNA_AVAILABLE:\n",
    "                    print('Running Optuna tuning (this may take a while)...')\n",
    "                    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "                    study.optimize(lambda trial: optuna_objective(trial, X_train_all_s, y_train_all,\n",
    "                                                                  n_splits=max(2, n_splits // 2),\n",
    "                                                                  initial_train_size=initial_train_size),\n",
    "                                   n_trials=n_trials)\n",
    "                    best_params = study.best_params\n",
    "                    print('Optuna best params:', best_params)\n",
    "                elif use_optuna and not OPTUNA_AVAILABLE:\n",
    "                    print('Optuna not installed. Running with default xgboost params.')\n",
    "\n",
    "                xgb_params = best_params or {\n",
    "                    'n_estimators': 500,\n",
    "                    'learning_rate': 0.05,\n",
    "                    'max_depth': 6,\n",
    "                    'subsample': 0.8,\n",
    "                    'colsample_bytree': 0.8,\n",
    "                    'random_state': SEED\n",
    "                }\n",
    "\n",
    "                # Evaluate with walk-forward CV\n",
    "                folds, _ = run_walk_forward_cv(X_train_all_s, y_train_all, n_splits=n_splits,\n",
    "                                               initial_train_size=initial_train_size, params=xgb_params)\n",
    "\n",
    "                # Train final model on full training set using chosen params\n",
    "                final_model = train_xgb(X_train_all_s, y_train_all, params=xgb_params)\n",
    "                preds_test = final_model.predict(X_test_s)\n",
    "                test_metrics = eval_metrics(y_test, preds_test)\n",
    "\n",
    "                model_results['xgb'] = {'cv_folds': folds, 'test_metrics': test_metrics, 'best_params': xgb_params}\n",
    "                meta = save_artifacts(final_model, scaler, out_dir=os.path.join(out_dir, t), name=f'{t}_xgb')\n",
    "                model_results['xgb']['saved'] = meta\n",
    "\n",
    "        if model_type in ('rf', 'all'):\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            rf = RandomForestRegressor(n_estimators=200, random_state=SEED, n_jobs=-1)\n",
    "            rf.fit(X_train_all_s, y_train_all)\n",
    "            preds = rf.predict(X_test_s)\n",
    "            model_results['rf'] = {'test_metrics': eval_metrics(y_test, preds)}\n",
    "            meta = save_artifacts(rf, scaler, out_dir=os.path.join(out_dir, t), name=f'{t}_rf')\n",
    "            model_results['rf']['saved'] = meta\n",
    "\n",
    "        summary[t] = model_results\n",
    "        print(f\"Finished target={t}. Summary:\")\n",
    "        print(json.dumps(model_results.get('xgb', model_results), indent=2, default=str))\n",
    "\n",
    "    # write summary\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    with open(os.path.join(out_dir, 'training_summary.json'), 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data', type=str, default='reliance.csv',\n",
    "                        help='Path to engineered features CSV (must contain Open and Close)')\n",
    "    parser.add_argument('--lags', type=int, nargs='+', default=[1, 2, 3, 5],\n",
    "                        help='Lag periods to generate (e.g. --lags 1 2 3 5)')\n",
    "    parser.add_argument('--model', type=str, default='xgb', choices=['xgb', 'rf', 'all'],\n",
    "                        help='Model to train (xgb recommended)')\n",
    "    parser.add_argument('--target', type=str, default='next_close', choices=['next_open', 'next_close', 'both'],\n",
    "                        help='Which target to train')\n",
    "    parser.add_argument('--test_size', type=float, default=0.2, help='Test set proportion (chronological)')\n",
    "    parser.add_argument('--preserve_time', action='store_true', help='Use chronological split (recommended)')\n",
    "    parser.add_argument('--out_dir', type=str, default='models', help='Output folder to save models and scalers')\n",
    "    parser.add_argument('--use_optuna', action='store_true', help='Enable Optuna tuning for XGBoost')\n",
    "    parser.add_argument('--n_trials', type=int, default=20, help='Number of Optuna trials')\n",
    "    parser.add_argument('--n_splits', type=int, default=5, help='Number of folds for walk-forward CV')\n",
    "    parser.add_argument('--initial_train_size', type=int, default=None,\n",
    "                        help='Initial train size (rows) for walk-forward CV (optional)')\n",
    "\n",
    "    # allow running inside Jupyter by ignoring unknown args Jupyter injects\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    run_pipeline(\n",
    "        data_path=args.data,\n",
    "        lags=args.lags,\n",
    "        model_type=args.model,\n",
    "        target=args.target,\n",
    "        test_size=args.test_size,\n",
    "        preserve_time=args.preserve_time,\n",
    "        out_dir=args.out_dir,\n",
    "        use_optuna=args.use_optuna,\n",
    "        n_trials=args.n_trials,\n",
    "        n_splits=args.n_splits,\n",
    "        initial_train_size=args.initial_train_size\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e040b-2471-4594-9c54-39493dbaed88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b18e51-02ab-454e-bdf2-de7405ffcc9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc8dfe5-da19-49fa-ab4f-316090df3cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3930bc15-740b-4c4c-87e0-94e633270d76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2390199b-8d06-42cd-91ce-57d25c48c0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2157f605-ed7f-43b3-9fdb-acac5c4cf96f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea1558c-0d9f-4989-8efc-e3289387b72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715a4fb5-d94e-4b3c-bf45-aad8266cbee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9142fcb-af67-4280-9314-e81374ce3694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027e0cc-975d-41d0-b953-1c306f65f90a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69098d57-a4b9-4934-bd43-301ab66276ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be30ebdd-9b43-4f30-a21f-21448fc5e5fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c47c268-a263-4eb2-a8cb-9d01426597af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b28b68-1e91-48cd-a6f6-dd3a7960043b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bff36f-9903-488a-abd1-eca92acb2e85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30ab448-b074-4c29-a5cc-2ae3a4f4556c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e9e2b0-1712-4be9-8b2d-cfae1b54e5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
